"""
å®Œå–„ç‰ˆè®­ç»ƒè„šæœ¬ï¼ˆCPU å‹å¥½ï¼Œèåˆæ¨¡å‹ï¼‰

é˜¶æ®µä¸€ï¼šè‡ªæˆ‘å¯¹å¼ˆæ•°æ®ç”Ÿæˆï¼ˆAlphaZeroGomokuAIï¼‰
é˜¶æ®µäºŒï¼šæ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–ï¼ˆæ•°æ®åŠ è½½ã€æ•°æ®å¢å¼ºã€è°ƒåº¦ã€æ—©åœã€æ¢¯åº¦è£å‰ªï¼‰
é˜¶æ®µä¸‰ï¼šæ¨¡å‹è¯„ä¼°ä¸éªŒè¯ï¼ˆèƒœç‡ä¸æŸå¤±ï¼‰

ä¿å­˜ç­–ç•¥ï¼š
- æ¯è¿­ä»£ä¿å­˜ models/alphazero_gomoku_iter_{i}.pth
- ç»´æŠ¤ models/alphazero_gomoku_best.pthï¼ˆåŸºäºéªŒè¯æŒ‡æ ‡ï¼‰
- æœ€ç»ˆä¿å­˜ models/alphazero_gomoku_final.pth

æ³¨ï¼šæœ¬è„šæœ¬ä¸“ä¸º CPU è¿è¡Œä¼˜åŒ–ï¼Œåˆç†æ§åˆ¶å¯¹å¼ˆæ•°é‡ä¸ MCTS æ¨¡æ‹Ÿæ¬¡æ•°ã€‚
"""

import os
import math
import random
import copy
import time
from typing import List, Tuple, Dict, Any, Optional

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from gomoku_board import GomokuBoard
from ai_agent import AlphaZeroGomokuAI
from neural_network import GomokuModel


# ----------------------------
# æ•°æ®å¢å¼ºï¼šæ£‹ç›˜çš„ 8 ç§å¯¹ç§°å˜æ¢
# ----------------------------

def _idx_to_rc(idx: int, n: int = 15) -> Tuple[int, int]:
    return idx // n, idx % n

def _rc_to_idx(r: int, c: int, n: int = 15) -> int:
    return r * n + c

def _transform_planes(planes: np.ndarray, k_rot: int, flip: bool) -> np.ndarray:
    # planes: (3, H, W)
    x = planes
    for _ in range(k_rot % 4):
        x = np.rot90(x, k=1, axes=(1, 2))
    if flip:
        x = np.flip(x, axis=2)  # æ°´å¹³ç¿»è½¬
    return x.copy()

def _transform_index(idx: int, k_rot: int, flip: bool, n: int = 15) -> int:
    r, c = _idx_to_rc(idx, n)
    # å°† (r,c) åº”ç”¨ç›¸åŒçš„å˜æ¢
    # æ—‹è½¬ 90 åº¦ï¼š (r, c) -> (c, n-1-r)
    for _ in range(k_rot % 4):
        r, c = c, n - 1 - r
    if flip:
        c = n - 1 - c
    return _rc_to_idx(r, c, n)

def augment_sample(planes: np.ndarray, move_idx: int) -> List[Tuple[np.ndarray, int]]:
    out = []
    for k_rot in range(4):
        for flip in (False, True):
            x = _transform_planes(planes, k_rot, flip)
            y = _transform_index(move_idx, k_rot, flip)
            out.append((x, y))
    return out


# ----------------------------
# ç®€å•ç»éªŒç¼“å†²åŒº
# ----------------------------

class SimpleReplay:
    def __init__(self):
        self.states: List[np.ndarray] = []      # (3, 15, 15)
        self.move_indices: List[int] = []       # 0..224
        self.players: List[int] = []            # 1 or 2
        self.outcomes: List[int] = []           # -1/0/1

    def add(self, planes: np.ndarray, move_idx: int, player: int):
        self.states.append(planes.astype(np.float32))
        self.move_indices.append(int(move_idx))
        self.players.append(int(player))

    def finalize_with_winner(self, winner: Optional[int]):
        for p in self.players:
            if winner is None:
                self.outcomes.append(0)
            else:
                self.outcomes.append(1 if p == winner else -1)

    def __len__(self) -> int:
        return len(self.states)


# ----------------------------
# Dataset ä¸ DataLoader
# ----------------------------

class GomokuSelfPlayDataset(Dataset):
    def __init__(self, replay: SimpleReplay, use_augmentation: bool = True, augment_ratio: float = 0.5):
        self.samples: List[Tuple[np.ndarray, int, float]] = []  # (planes, move_idx, value)
        n = len(replay)
        indices = list(range(n))
        # å°†åŸå§‹æ ·æœ¬çº³å…¥
        for i in indices:
            planes = replay.states[i]
            move_idx = replay.move_indices[i]
            value = float(replay.outcomes[i])
            self.samples.append((planes, move_idx, value))
        # æ•°æ®å¢å¼ºï¼šæŒ‰æ¯”ä¾‹å¯¹æ ·æœ¬åš 8 å€å¯¹ç§°å¢å¼ºï¼ˆå­é›†ï¼‰
        if use_augmentation and n > 0:
            aug_count = int(n * augment_ratio)
            chosen = random.sample(indices, k=max(1, aug_count))
            for i in chosen:
                planes = replay.states[i]
                move_idx = replay.move_indices[i]
                value = float(replay.outcomes[i])
                for aug_planes, aug_idx in augment_sample(planes, move_idx):
                    self.samples.append((aug_planes, aug_idx, value))

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int):
        planes, move_idx, value = self.samples[idx]
        x = torch.from_numpy(planes)            # (3,15,15)
        y_policy = torch.tensor(move_idx, dtype=torch.long)
        y_value = torch.tensor([value], dtype=torch.float32)
        return x, y_policy, y_value


# ----------------------------
# è‡ªæˆ‘å¯¹å¼ˆ + è¯„ä»·
# ----------------------------

def play_one_game(ai_black: AlphaZeroGomokuAI, ai_white: AlphaZeroGomokuAI, 
                  step_timeout: float = 10.0, game_timeout: float = 300.0) -> Tuple[SimpleReplay, int]:
    """
    è¿›è¡Œä¸€å±€è‡ªæˆ‘å¯¹å¼ˆï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
    å‚æ•°:
        ai_black: é»‘å­AI
        ai_white: ç™½å­AI
        step_timeout: å•æ­¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        game_timeout: æ•´å±€è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    """
    board = GomokuBoard()
    buf = SimpleReplay()
    start_moves = board.get_move_count()
    game_start_time = time.time()
    step_count = 0
    slow_steps = 0  # æ…¢æ­¥è®¡æ•°

    while not board.game_over:
        current_time = time.time()
        game_elapsed = current_time - game_start_time
        
        # æ•´å±€è¶…æ—¶ä¿æŠ¤
        if game_elapsed > game_timeout:
            print(f"\r    â”œâ”€â”€ å¯¹å±€è¶…æ—¶: {step_count}æ­¥ [æ€»ç”¨æ—¶:{game_elapsed:.1f}s] âš ï¸", end="")
            break
            
        current_player = board.current_player
        ai = ai_black if current_player == GomokuBoard.BLACK else ai_white
        
        # å•æ­¥è¶…æ—¶ä¿æŠ¤
        step_start = time.time()
        try:
            move = ai.get_move(board)
            step_elapsed = time.time() - step_start
            
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if step_elapsed > step_timeout:
                print(f"\r    â”œâ”€â”€ æ­¥{step_count+1}è¶…æ—¶: {step_elapsed:.1f}s > {step_timeout}s âš ï¸", end="")
                # ä½¿ç”¨éšæœºç§»åŠ¨ç»§ç»­
                valid_moves = board.get_valid_moves()
                if valid_moves:
                    move = random.choice(valid_moves)
                else:
                    break
            elif step_elapsed > 3.0:
                slow_steps += 1
                print(f"\r    â”œâ”€â”€ å¯¹å±€{step_count+1}: æ­¥æ•°:{step_count+1} [æ€»ç”¨æ—¶:{game_elapsed:.1f}s] æ…¢æ­¥è­¦å‘Šâš ï¸", end="")
            else:
                print(f"\r    â”œâ”€â”€ å¯¹å±€{step_count+1}: æ­¥æ•°:{step_count+1} [æ€»ç”¨æ—¶:{game_elapsed:.1f}s]", end="")
                
        except Exception as e:
            print(f"\r    â”œâ”€â”€ æ­¥{step_count+1}å¼‚å¸¸: {str(e)[:30]}... âš ï¸", end="")
            # ä½¿ç”¨éšæœºç§»åŠ¨ç»§ç»­
            valid_moves = board.get_valid_moves()
            if valid_moves:
                move = random.choice(valid_moves)
            else:
                break
        
        if move is None:
            break
            
        # è®°å½•å½“å‰çŠ¶æ€
        planes = board.get_board_tensor()
        move_idx = move[0] * board.BOARD_SIZE + move[1]
        buf.add(planes, move_idx, current_player)
        board.make_move(move[0], move[1])
        step_count += 1

    buf.finalize_with_winner(board.winner)
    game_len = board.get_move_count() - start_moves
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    final_time = time.time() - game_start_time
    winner_text = "ç©å®¶1èƒœ" if board.winner == GomokuBoard.BLACK else "ç©å®¶2èƒœ" if board.winner == GomokuBoard.WHITE else "å¹³å±€"
    print(f"\r    â”œâ”€â”€ å¯¹å±€å®Œæˆ: {game_len}æ­¥ {winner_text} [ç”¨æ—¶:{final_time:.1f}s]")
    
    return buf, game_len


def evaluate_model(current: GomokuModel, baseline: Optional[GomokuModel], device: torch.device,
                   games: int = 8, eval_difficulty: str = "easy", eval_num_sim: int = 60,
                   eval_plans: int = 2) -> Dict[str, Any]:
    # å¦‚æœæ²¡æœ‰åŸºçº¿ï¼Œä½¿ç”¨éšæœºç€æ³•ä½œä¸ºå¯¹æ‰‹
    class RandomAgent:
        def get_move(self, board: GomokuBoard):
            v = board.get_valid_moves()
            return random.choice(v) if v else None

    black_is_current = True
    wins = 0
    losses = 0
    draws = 0

    for g in range(games):
        board = GomokuBoard()
        color_a = GomokuBoard.BLACK if black_is_current else GomokuBoard.WHITE
        if baseline is None:
            ai_a = AlphaZeroGomokuAI(color_a, difficulty=eval_difficulty, device=str(device), planner_steps=eval_plans)
            ai_a.params["num_simulations"] = eval_num_sim
            ai_b = RandomAgent()
        else:
            ai_a = AlphaZeroGomokuAI(color_a, difficulty=eval_difficulty, device=str(device), planner_steps=eval_plans)
            other_color = GomokuBoard.WHITE if color_a == GomokuBoard.BLACK else GomokuBoard.BLACK
            ai_b = AlphaZeroGomokuAI(other_color, difficulty=eval_difficulty, device=str(device), planner_steps=eval_plans)
            ai_b.params["num_simulations"] = eval_num_sim
            ai_b.model = baseline
        ai_a.model = current

        while not board.game_over:
            current_player = board.current_player
            agent = ai_a if (current_player == GomokuBoard.BLACK if black_is_current else current_player == GomokuBoard.WHITE) else ai_b
            move = agent.get_move(board)
            if move is None:
                break
            board.make_move(*move)

        if board.winner is None:
            draws += 1
        else:
            if board.winner == color_a:
                wins += 1
            else:
                losses += 1

        # äº¤æ›¿å…ˆæ‰‹åˆ°ä¸‹ä¸€å±€
        black_is_current = not black_is_current

    total = max(1, wins + losses + draws)
    return {"wins": wins, "losses": losses, "draws": draws, "win_rate": wins / total}


# ----------------------------
# è®­ç»ƒä¸€ä¸ª epoch
# ----------------------------

def train_epoch(model: GomokuModel, loader: DataLoader, optimizer: optim.Optimizer, device: torch.device,
                grad_clip: float = 1.0, epoch_index: int = 1, num_epochs: int = 1) -> float:
    model.train_mode()
    ce = nn.CrossEntropyLoss()
    mse = nn.MSELoss()
    total_loss = 0.0
    batches = 0

    total_batches = len(loader)
    processed = 0
    for x, y_policy, y_value in loader:
        x = x.to(device)
        y_policy = y_policy.to(device)
        y_value = y_value.to(device)

        optimizer.zero_grad()
        policy_logits, value_pred = model.model(x)
        loss_policy = ce(policy_logits, y_policy)
        loss_value = mse(value_pred, y_value)
        loss = loss_policy + loss_value
        loss.backward()
        if grad_clip is not None and grad_clip > 0:
            nn.utils.clip_grad_norm_(model.model.parameters(), grad_clip)
        optimizer.step()

        total_loss += float(loss.item())
        batches += 1
        processed += 1
        # Epochçº§åˆ«è¿›åº¦æ¡ï¼ˆå•æ¡ï¼‰
        pct = processed / max(1, total_batches)
        bar = _render_bar(pct, width=12)
        print(f"\r    â”œâ”€â”€ æ¨¡å‹è®­ç»ƒ: {bar} {int(pct*100)}% (epoch {epoch_index}/{num_epochs})", end="")

    print("")
    return total_loss / max(1, batches)


def validate_epoch(model: GomokuModel, loader: DataLoader, device: torch.device, epoch_index: int = 1, num_epochs: int = 1) -> float:
    model.eval_mode()
    ce = nn.CrossEntropyLoss()
    mse = nn.MSELoss()
    total_loss = 0.0
    batches = 0
    total_batches = len(loader)
    processed = 0
    with torch.no_grad():
        for x, y_policy, y_value in loader:
            x = x.to(device)
            y_policy = y_policy.to(device)
            y_value = y_value.to(device)
            policy_logits, value_pred = model.model(x)
            loss = ce(policy_logits, y_policy) + mse(value_pred, y_value)
            total_loss += float(loss.item())
            batches += 1
            processed += 1
            # éªŒè¯è¿›åº¦æ¡ï¼ˆå•æ¡ï¼‰
            pct = processed / max(1, total_batches)
            bar = _render_bar(pct, width=12)
            print(f"\r    â”œâ”€â”€ éªŒè¯ä¸­:   {bar} {int(pct*100)}% (epoch {epoch_index}/{num_epochs})", end="")
    print("")
    return total_loss / max(1, batches)


# ----------------------------
# è¿›åº¦æ¸²æŸ“å·¥å…·
# ----------------------------

def _render_bar(pct: float, width: int = 20) -> str:
    pct = max(0.0, min(1.0, pct))
    filled = int(pct * width)
    empty = width - filled
    return "â–ˆ" * filled + "â–‘" * empty

def _fmt_duration(seconds: float) -> str:
    seconds = int(max(0, seconds))
    h = seconds // 3600
    m = (seconds % 3600) // 60
    return f"{h}å°æ—¶{m}åˆ†"


# ----------------------------
# ä¸»æµç¨‹
# ----------------------------

def main():
    device = torch.device("cpu")
    os.makedirs("models", exist_ok=True)

    print("ğŸš€ å¼€å§‹ AlphaZero äº”å­æ£‹è®­ç»ƒ")
    print("=" * 50)
    print(f"è®¾å¤‡: {device}")
    print(f"æ¨¡å‹ç›®å½•: {os.path.abspath('models')}")

    # åŠ è½½æˆ–åˆå§‹åŒ–æ¨¡å‹ï¼ˆå†…éƒ¨ä¼šä¼˜å…ˆ best -> æœ€æ–° -> éšæœºï¼‰
    print("ğŸ“¦ åŠ è½½æ¨¡å‹ä¸­...")
    model = GomokuModel(model_path=None, board_size=15, device=str(device))
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # è®­ç»ƒé…ç½®ï¼ˆCPU å‹å¥½ï¼‰
    iterations = 6            # æ€»è¿­ä»£è½®æ•°
    games_per_iteration = 10  # æ¯è½®è‡ªæˆ‘å¯¹å¼ˆå±€æ•°ï¼ˆå‡å°ä»¥æé€Ÿï¼‰
    train_split = 0.9         # æ›´å¤šæ ·æœ¬ç”¨äºè®­ç»ƒ
    batch_size = 128         # æ‰¹æ¬¡
    learning_rate = 8e-4
    grad_clip = 0.8

    print(f"è®­ç»ƒé…ç½®: {iterations}è½®è¿­ä»£, æ¯è½®{games_per_iteration}å±€å¯¹å¼ˆ")
    print("=" * 50)

    # ä¼˜åŒ–ä¸è°ƒåº¦
    optimizer = optim.Adam(model.model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.85)

    # æ—©åœæœºåˆ¶
    patience = 3
    best_val = math.inf
    patience_count = 0

    # è¯„ä¼°ç”¨åŸºçº¿ï¼ˆä¸Šä¸€è½®æœ€ä½³çš„å¿«ç…§ï¼‰
    best_model_snapshot: Optional[GomokuModel] = None

    t0 = time.time()


    for it in range(1, iterations + 1):
        # é˜¶æ®µä¸€ï¼šè‡ªæˆ‘å¯¹å¼ˆæ•°æ®ç”Ÿæˆï¼ˆå¸¦æ—¶é—´é™åˆ¶ï¼‰
        ai_black = AlphaZeroGomokuAI(GomokuBoard.BLACK, difficulty="medium", device=str(device), 
                                    time_limit=5.0, time_reward_factor=0.1)
        ai_white = AlphaZeroGomokuAI(GomokuBoard.WHITE, difficulty="medium", device=str(device),
                                    time_limit=5.0, time_reward_factor=0.1)
        ai_black.model = model
        ai_white.model = model

        replay = SimpleReplay()
        finished_games = 0
        total_moves_acc = 0
        # è‡ªæˆ‘å¯¹å¼ˆè¿›åº¦ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
        print(f"ğŸ”„ è¿­ä»£ {it}/{iterations} - é˜¶æ®µä¸€ï¼šè‡ªæˆ‘å¯¹å¼ˆæ•°æ®ç”Ÿæˆ")
        for g in range(games_per_iteration):
            buf, glen = play_one_game(ai_black, ai_white, step_timeout=8.0, game_timeout=180.0)
            replay.states.extend(buf.states)
            replay.move_indices.extend(buf.move_indices)
            replay.players.extend(buf.players)
            replay.outcomes.extend(buf.outcomes)
            finished_games += 1
            total_moves_acc += glen
            pg_pct = finished_games / games_per_iteration
            bar = _render_bar(pg_pct, width=20)
            avg_moves = int(total_moves_acc/max(1,finished_games))
            print(f"\r    â”œâ”€â”€ è‡ªæˆ‘å¯¹å¼ˆ: {bar} {int(pg_pct*100)}% ({finished_games}/{games_per_iteration}å±€) å¹³å‡æ­¥æ•°â‰ˆ{avg_moves}", end="")
        print("")

        # é˜¶æ®µäºŒï¼šæ•°æ®åŠ è½½ä¸è®­ç»ƒï¼ˆå«å¢å¼ºä¸éªŒè¯ï¼‰
        dataset = GomokuSelfPlayDataset(replay, use_augmentation=True, augment_ratio=0.35)
        if len(dataset) < 8:
            print("è‡ªæˆ‘å¯¹å¼ˆæ ·æœ¬è¿‡å°‘ï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒã€‚")
            continue
        # åˆ’åˆ†è®­ç»ƒ/éªŒè¯
        n = len(dataset)
        idx = list(range(n))
        random.shuffle(idx)
        split = int(n * train_split)
        train_idx = idx[:split]
        val_idx = idx[split:]

        class Subset(Dataset):
            def __init__(self, base: Dataset, indices: List[int]):
                self.base = base
                self.indices = indices
    
            def __len__(self):
                return len(self.indices)

            def __getitem__(self, i):
                return self.base[self.indices[i]]

        train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size, shuffle=False)

        # è®­ç»ƒè‹¥å¹² epochï¼ˆè¿™é‡Œæ¯è½® 2 æ¬¡ï¼‰ï¼Œæ¯ä¸ªepochæ˜¾ç¤ºå•æ¡è¿›åº¦æ¡
        train_losses = []
        val_losses = []
        epochs = 2
        for ep in range(epochs):
            # è¿™é‡Œåœ¨ train_epoch å†…æ˜¾ç¤ºæ‰¹æ¬¡æ¡ï¼›æ­¤å¤„æ˜¾ç¤ºæ€»ä½“epochè¿›åº¦
            train_loss = train_epoch(model, train_loader, optimizer, device, grad_clip=grad_clip, epoch_index=ep+1, num_epochs=epochs)
            val_loss = validate_epoch(model, val_loader, device, epoch_index=ep+1, num_epochs=epochs)
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            ep_pct = (ep + 1) / epochs
            ep_bar = _render_bar(ep_pct, width=12)
            print(f"    â”œâ”€â”€ æ¨¡å‹è®­ç»ƒ: {ep_bar} {int(ep_pct*100)}% (epoch {ep+1}/{epochs})")
        avg_train = float(np.mean(train_losses))
        avg_val = float(np.mean(val_losses))

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        # ä¿å­˜å½“å‰è¿­ä»£æ¨¡å‹
        iter_path = os.path.join("models", f"alphazero_gomoku_iter_{it}.pth")
        model.save_model(iter_path)

        # æ—©åœä¸æœ€ä½³æ¨¡å‹ç»´æŠ¤ï¼ˆä»¥éªŒè¯æŸå¤±ä¸ºå‡†ï¼‰
        improved = avg_val < best_val
        if improved:
            best_val = avg_val
            patience_count = 0
            best_path = os.path.join("models", "alphazero_gomoku_best.pth")
            model.save_model(best_path)
            # å¤åˆ¶å¿«ç…§ç”¨äºè¯„ä¼°å¯¹å±€
            best_model_snapshot = GomokuModel(model_path=best_path, board_size=15, device=str(device))
        else:
            patience_count += 1

        # é˜¶æ®µä¸‰ï¼šæ¨¡å‹è¯„ä¼°ï¼ˆå¯¹æˆ˜è¯„ä¼°ï¼‰
        # è¯„ä¼°è¿›åº¦ï¼ˆæŒ‰å¯¹å±€æ•°æ˜¾ç¤ºï¼‰
        eval_games = 6
        wins = 0
        losses = 0
        draws = 0
        for eg in range(eval_games):
            # å•å±€è¯„ä¼°
            stats = evaluate_model(model, best_model_snapshot, device=device, games=1)
            wins += stats.get('wins', 0)
            losses += stats.get('losses', 0)
            draws += stats.get('draws', 0)
            pct = (eg + 1) / eval_games
            bar = _render_bar(pct, width=12)
            print(f"\r    â””â”€â”€ æ¨¡å‹è¯„ä¼°: {bar} {int(pct*100)}% ({eg+1}/{eval_games})", end="")
        print("")
        total = max(1, wins + losses + draws)
        eval_stats = {"wins": wins, "losses": losses, "draws": draws, "win_rate": wins / total}

        # é¡¶éƒ¨æ€»ä½“è¿›åº¦ä¸æ—¶é—´ä¼°è®¡
        overall_pct = it / iterations
        elapsed = time.time() - t0
        eta = (elapsed / max(1e-6, overall_pct)) * (1 - overall_pct)
        overall_bar = _render_bar(overall_pct, width=20)
        avg_game_len = int(total_moves_acc / max(1, finished_games))
        print(f"ğŸ“Š æ€»ä½“è¿›åº¦: {overall_bar} {int(overall_pct*100)}%")
        print(f"    â±ï¸  å·²è¿è¡Œ: {_fmt_duration(elapsed)} | é¢„è®¡å‰©ä½™: {_fmt_duration(eta)}")
        print(f"\n    ğŸ”„ å½“å‰é˜¶æ®µ: æ¨¡å‹è®­ç»ƒ (è¿­ä»£ {it}/{iterations})")
        print(f"    â”œâ”€â”€ è‡ªæˆ‘å¯¹å¼ˆ: âœ… å®Œæˆ ({finished_games}/{games_per_iteration}å±€)")
        print(f"    â”œâ”€â”€ æ¨¡å‹è®­ç»ƒ: âœ… å®Œæˆ (epoch {epochs}/{epochs})")
        print(f"    â””â”€â”€ æ¨¡å‹è¯„ä¼°: âœ… å®Œæˆ")

        # æ€§èƒ½æŒ‡æ ‡å±•ç¤ºï¼ˆæ—  Elo ä¼°è®¡åˆ™çœç•¥å¢é‡ï¼Œä»…æ˜¾ç¤ºå½“å‰ï¼‰
        wr = eval_stats.get('win_rate', 0.0) * 100.0
        print("\n    ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        print(f"    â”œâ”€â”€ èƒœç‡: {wr:.1f}%")
        print(f"    â”œâ”€â”€ å¹³å‡æ¸¸æˆé•¿åº¦: {avg_game_len}æ­¥")
        print(f"    â””â”€â”€ æŸå¤±å€¼: {avg_val:.3f}")

        if patience_count >= patience:
            print("è§¦å‘æ—©åœæ¡ä»¶ï¼Œç»“æŸè®­ç»ƒã€‚")
            break

    # æœ€ç»ˆä¿å­˜
    final_path = os.path.join("models", "alphazero_gomoku_final.pth")
    model.save_model(final_path)
    print(f"Saved final model to {final_path}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
